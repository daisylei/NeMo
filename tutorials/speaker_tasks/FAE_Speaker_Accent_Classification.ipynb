{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreign Accented English Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install unidecode\n",
    "\n",
    "# ## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "## Install TorchAudio\n",
    "!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoken communication shares information conveyed across multiple channels.\n",
    "Traveling along with the language, paralinguistic attributes supplement the spoken message with information that can enrich the semantic content beyond the spoken words.\n",
    "\n",
    "This tutorial demonstrates how to classify a speech recording (in English) into one foreign-accent class.\n",
    "\n",
    "For this demonstration we will be using the [FAE Common Voice 2022](https://github.com/schaltung/FAE-CV) freely available on GitHub. This is a modified version of the crowdsourced corpus [Common Voice corpus](https://commonvoice.mozilla.org/en).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT, \"data\")\n",
    "fae_cv_url = \"https://github.com/schaltung/FAE-CV/blob/026e3128d4af46cb3fd3a16c7d97e2cbda0d079c/logs/2022-09-06_00-51-39/\"\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "for tsv_file in ('train.tsv', 'eval.tsv', 'dev.tsv'):\n",
    "    tsv_path = os.path.join(data_dir, \"train.tsv\")\n",
    "    tsv_url = f'{fae_cv_url}/{tsv_file}'\n",
    "    if not os.path.exists(tsv_path):\n",
    "        print(f'downloading {tsv_url}')\n",
    "        wget.download(tsv_url, data_dir)\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a Speaker-Recognition pretrained model.\n",
    "\n",
    "##### Repurpose a DNN pretrained for speaker separation to learn an embedding space where accents are separated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings with TensorBoard Projector\n",
    "\n",
    "#### Task: Foreign Accent English classification\n",
    "\n",
    "#### Corpus: Common Voice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants Definition\n",
    "\n",
    "- Model\n",
    "- Data manifests\n",
    "- Output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data1/nemo_experiments\"\n",
    "EXP_DIR = f\"{BASE_DIR}/220831-Finetune+Eval-CV-validated\"\n",
    "EXP_DIR0 = f\"{BASE_DIR}/220823-Finetune-CV-validated/220825-Finetune-CV-eval\"\n",
    "LOG_DIR = f\"{BASE_DIR}/tb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_nemo_path = (\n",
    "    f\"{BASE_DIR}/tb/Finetune-CV/2022-08-27_02-58-59/checkpoints/Finetune-CV.nemo\"\n",
    ")\n",
    "#\n",
    "# Data manifests\n",
    "eval_manifest_filepath = f\"{EXP_DIR}/cv-self-manifest.json\"\n",
    "\n",
    "eval0_manifest_filepath = (\n",
    "    f\"{EXP_DIR0}/antonio_validated-reduced-resampled-eval_manifest.json\"\n",
    ")\n",
    "\n",
    "#\n",
    "# The problem with the data below is that the label space is different.\n",
    "# wcat_seg_manifest_filepath = f'{BASE_DIR}/WildcatDiapix/filelist_manifest-2.json'\n",
    "# wcat_all_manifest_filepath = f'{BASE_DIR}/WildcatDiapix/filelist_manifest.json'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define In/Out filepaths\n",
    "# Select below as needed.\n",
    "#\n",
    "conf = OmegaConf.create()\n",
    "conf[\"data\"] = {}\n",
    "# IN:\n",
    "conf.data[\"eval\"] = dict(manifest_path=eval_manifest_filepath)\n",
    "conf.data.eval[\"title\"] = \"%s\\n%s\" % (\n",
    "    model_nemo_path.replace(BASE_DIR + \"/\", \"\"),\n",
    "    os.path.basename(conf.data.eval.manifest_path),\n",
    ")\n",
    "#\n",
    "conf.data[\"eval0\"] = dict(manifest_path=eval0_manifest_filepath)\n",
    "conf.data.eval0[\"title\"] = \"%s\\n%s\" % (\n",
    "    model_nemo_path.replace(BASE_DIR + \"/\", \"\"),\n",
    "    os.path.basename(conf.data.eval0.manifest_path),\n",
    ")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1: Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-08-31 12:37:59 modelPT:149] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: nemo_experiments/220823-Finetune-CV-validated/220825-Finetune-CV-eval/antonio_validated-reduced-resampled-train_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - african\n",
      "    - australian\n",
      "    - canada\n",
      "    - england\n",
      "    - hongkong\n",
      "    - india\n",
      "    - ireland\n",
      "    - newzealand\n",
      "    - philippines\n",
      "    - scotland\n",
      "    - us\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    augmentor:\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    \n",
      "[NeMo W 2022-08-31 12:37:59 modelPT:156] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: nemo_experiments/220823-Finetune-CV-validated/220825-Finetune-CV-eval/antonio_validated-reduced-resampled-dev_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2022-08-31 12:37:59 modelPT:162] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: nemo_experiments/220823-Finetune-CV-validated/220825-Finetune-CV-eval/antonio_validated-reduced-resampled-eval_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-08-31 12:37:59 features:200] PADDING: 16\n",
      "[NeMo I 2022-08-31 12:38:00 label_models:98] loss is Angular Softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-08-31 12:38:00 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (TopKClassificationAccuracy). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-08-31 12:38:00 save_restore_connector:243] Model EncDecSpeakerLabelModel was successfully restored from /data1/nemo_experiments/tb/Finetune-CV/2022-08-27_02-58-59/checkpoints/Finetune-CV.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load DNN.\n",
    "speaker_model = EncDecSpeakerLabelModel.restore_from(model_nemo_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Extract Embeddings\n",
    "\n",
    "#### Evaluation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-08-31 12:38:13 collections:290] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2022-08-31 12:38:13 collections:294] # 24 files loaded accounting to # 2 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.1: Run Evaluation.\n",
    "#\n",
    "MANIFEST_FILEPATH = conf.data.eval.manifest_path\n",
    "#\n",
    "(\n",
    "    eval_embs,\n",
    "    eval_logits,\n",
    "    eval_ref_labels,\n",
    "    eval_idx2labD,\n",
    ") = EncDecSpeakerLabelModel.get_batch_embeddings(\n",
    "    speaker_model=speaker_model,\n",
    "    manifest_filepath=MANIFEST_FILEPATH,\n",
    "    batch_size=32,\n",
    "    sample_rate=16000,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "eval_embs = eval_embs / (np.linalg.norm(eval_embs, ord=2, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-08-31 12:39:03 collections:290] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2022-08-31 12:39:03 collections:294] # 2563 files loaded accounting to # 11 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:05<00:00, 15.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# STEP 2.1: Run Evaluation.\n",
    "#\n",
    "MANIFEST_FILEPATH = conf.data.eval0.manifest_path\n",
    "#\n",
    "(\n",
    "    eval0_embs,\n",
    "    eval0_logits,\n",
    "    eval0_ref_labels,\n",
    "    eval0_idx2labD,\n",
    ") = EncDecSpeakerLabelModel.get_batch_embeddings(\n",
    "    speaker_model=speaker_model,\n",
    "    manifest_filepath=MANIFEST_FILEPATH,\n",
    "    batch_size=32,\n",
    "    sample_rate=16000,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "eval0_embs = eval0_embs / (np.linalg.norm(eval0_embs, ord=2, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Visualization of the Embedding Space\n",
    "\n",
    "#### TensorBoard Projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch this to True if you really want to create a new entry in TB.\n",
    "WRITE2TB = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Eval Data Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather vector data\n",
    "vectors = eval_embs\n",
    "metadata = [eval_idx2labD[x] for x in eval_ref_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 24 embeddings in a 192-dim space.\n"
     ]
    }
   ],
   "source": [
    "# Create SummaryWritter\n",
    "logdir = f\"{LOG_DIR}/SelfEmbsEvalCV_{datetime.datetime.now().isoformat()}\".replace(\n",
    "    \":\", \"-\"\n",
    ")\n",
    "if WRITE2TB:\n",
    "    writer = SummaryWriter(logdir)\n",
    "    writer = SummaryWriter(logdir)\n",
    "    writer.add_embedding(vectors, metadata)\n",
    "    writer.close()\n",
    "print(f\"Wrote {vectors.shape[0]} embeddings in a {vectors.shape[1]}-dim space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experimentsl (skip this part below).\n",
    "\n",
    "###### Trying to add more meta-data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANIFEST_FILEPATH = conf.data.eval.manifest_path\n",
    "manifestDF = pd.read_json(MANIFEST_FILEPATH, lines=True)\n",
    "manifestDF[\"path\"] = manifestDF.audio_filepath.apply(lambda x: x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "origMetaDF = pd.read_csv(\n",
    "    f\"{BASE_DIR}/220831-Finetune+Eval-CV-validated/CV-self-recordings/data.tsv\", sep=\",\"\n",
    ")\n",
    "# origMetaDF\n",
    "# origMetaDF.columns=['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'locale', 'segment', 'label']\n",
    "#\n",
    "_ = pd.merge(left=manifestDF, right=origMetaDF, how=\"left\", on=[\"path\", \"label\"])\n",
    "_[\"speaker\"] = _.client_id\n",
    "_[\"collection\"] = \"iact\"\n",
    "metadataLOL = _[\n",
    "    [\"label\", \"gender\", \"age\", \"speaker\", \"sentence\", \"collection\"]\n",
    "].values.tolist()\n",
    "# metadataLOL = manifestDF[['label', 'path']].values.tolist()\n",
    "# metadataLOL\n",
    "MANIFEST_FILEPATH = conf.data.eval0.manifest_path\n",
    "manifest0DF = pd.read_json(MANIFEST_FILEPATH, lines=True)\n",
    "manifest0DF[\"path\"] = manifest0DF.audio_filepath.apply(\n",
    "    lambda x: x.split(\"/\")[-1]\n",
    ").apply(lambda x: x.replace(\".wav\", \".mp3\"))\n",
    "\n",
    "origMeta0DF = pd.read_csv(\n",
    "    f\"{BASE_DIR}/220823-Finetune-CV-validated/validated-label-dur.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    ")\n",
    "origMeta0DF.columns = [\n",
    "    \"client_id\",\n",
    "    \"path\",\n",
    "    \"sentence\",\n",
    "    \"up_votes\",\n",
    "    \"down_votes\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"accents\",\n",
    "    \"locale\",\n",
    "    \"segment\",\n",
    "    \"label\",\n",
    "    \"duration\",\n",
    "]\n",
    "#\n",
    "_ = pd.merge(left=manifest0DF, right=origMeta0DF, how=\"left\", on=[\"path\", \"label\"])\n",
    "_[\"speaker\"] = _.client_id.apply(lambda x: x[-4:])\n",
    "_[\"collection\"] = \"mozilla\"\n",
    "metadata0LOL = _[\n",
    "    [\"label\", \"gender\", \"age\", \"speaker\", \"sentence\", \"collection\"]\n",
    "].values.tolist()\n",
    "\n",
    "metadataLOL += metadata0LOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.concatenate((eval_embs, eval0_embs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather vector data\n",
    "vectors = embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/nemo_experiments/tb/SelfEmbsEvalCV_2022-08-31T13-09-03.226288\n",
      "Wrote 2587 embeddings in a 192-dim space.\n"
     ]
    }
   ],
   "source": [
    "# Create SummaryWritter\n",
    "logdir = f\"{LOG_DIR}/SelfEmbsEvalCV_{datetime.datetime.now().isoformat()}\".replace(\n",
    "    \":\", \"-\"\n",
    ")\n",
    "WRITE2TB = True\n",
    "if WRITE2TB:\n",
    "    writer = SummaryWriter(logdir)\n",
    "    writer.add_embedding(\n",
    "        vectors,\n",
    "        metadataLOL,\n",
    "        metadata_header=[\"label\", \"gender\", \"age\", \"speaker\", \"sentence\", \"collection\"],\n",
    "        tag=\"SelfEmbsEvalCV\",\n",
    "    )\n",
    "    writer.close()\n",
    "print(\n",
    "    f\"{logdir}\\nWrote {vectors.shape[0]} embeddings in a {vectors.shape[1]}-dim space.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
