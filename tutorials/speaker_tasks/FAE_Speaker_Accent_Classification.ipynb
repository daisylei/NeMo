{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreign Accented English Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsox-fmt-all is already the newest version (14.4.2+git20190427-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (59.5.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.0.7)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.12)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "BRANCH = 'fae/cv22'\n",
    "\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "if RunningInCOLAB:\n",
    "    ## Install dependencies\n",
    "    %pip install wget\n",
    "    %apt-get install sox libsndfile1 ffmpeg libsox-fmt-all -y\n",
    "\n",
    "    % python -m pip install unidecode\n",
    "\n",
    "    % python -m pip install git+https://github.com/schaltung/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]\n",
    "else:\n",
    "    ! apt-get update\n",
    "    ! apt-get install libsox-fmt-all -y\n",
    "    %pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoken communication travels across multiple channels. Alongside the language, paralinguistic attributes supplement spoken messages with information that enriches the semantic content beyond the spoken words. Paralinguistics technology is concerned with non-linguistic phenomena that encode attributes revealing speaker traits and states; some short-lived such as emotion, mood and health; while others are long-lived such as age, gender, accent, and ultimately the personal identity itself.\n",
    "\n",
    "This tutorial demonstrates how to classify a speech recording (in English) into one foreign-accent class.\n",
    "\n",
    "For this demonstration we will be using the [FAE Common Voice 2022](https://github.com/schaltung/FAE-CV) freely available on GitHub. This is a modified version of the crowdsourced corpus [Common Voice corpus](https://commonvoice.mozilla.org/en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:10:38 optimizers:77] Could not import distributed_fused_adam optimizer from Apex\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import json\n",
    "import wget\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import sox\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Preparation\n",
    "### Locate the Common Voice corpus.\n",
    "\n",
    "Confirm there is a local copy of Common-Voice, download otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing clips folder: /corpora/cv-corpus-10.0-2022-07-04/en/clips\n"
     ]
    }
   ],
   "source": [
    "# Local copy of Common-Voice corpus.\n",
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT, \"data\")\n",
    "CORPUS_NAME = 'cv-corpus-10.0-2022-07-04'\n",
    "LANGUAGE = 'en'\n",
    "#\n",
    "DIR_CORPORA = f'/corpora/{CORPUS_NAME}/{LANGUAGE}'\n",
    "COMMON_VOICE_URL = f\"https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/{CORPUS_NAME}/{CORPUS_NAME}-{LANGUAGE}.tar.gz\"\n",
    "clips_dir = f'{DIR_CORPORA}/clips'\n",
    "\n",
    "if os.path.exists(clips_dir):\n",
    "    print(f\"Found existing clips folder: {clips_dir}\")\n",
    "else:\n",
    "    print(f\"Could not find Common Voice, downloading corpus...\")\n",
    "    import subprocess, tarfile\n",
    "    output_archive_filename = f\"{data_dir}/cv-en.tar.gz\"\n",
    "    commands = [\n",
    "        \"wget\",\n",
    "        \"--user-agent\",\n",
    "        '\"Mozilla/5.0 (Windows NT 10.0; WOW64) '\n",
    "        'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\"',\n",
    "        \"-O\",\n",
    "        output_archive_filename,\n",
    "        f\"{COMMON_VOICE_URL}\",\n",
    "    ]\n",
    "    commands = \" \".join(commands)\n",
    "    subprocess.run(\n",
    "        commands, shell=True, stderr=sys.stderr, stdout=sys.stdout, capture_output=False\n",
    "    )\n",
    "    tar = tarfile.open(output_archive_filename)\n",
    "    try:\n",
    "        tar.extractall(DIR_CORPORA)\n",
    "        print(f\"extracted to: {DIR_CORPORA}\")\n",
    "    except:\n",
    "        print(f\"unable to extract to: {DIR_CORPORA}, trying {data_dir} instead.\")\n",
    "        DIR_CORPORA = data_dir\n",
    "        clips_dir = f'{DIR_CORPORA}/clips'\n",
    "        tar.extractall(DIR_CORPORA)\n",
    "        print(f\"extracted to: {DIR_CORPORA}\")\n",
    "    tar.close()\n",
    "    os.remove(output_archive_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the FAE-CV22 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the FAE-CV22 list files.\n",
    "# target location to store the slected wav files.\n",
    "wavs_dir =  os.path.join(data_dir, \"wavs\")\n",
    "fae_cv_url = \"https://raw.githubusercontent.com/schaltung/FAE-CV/9fc285d46834e6b5f43d2eb2eee52fce8e2723f0/logs/2022-09-06_00-51-39\"\n",
    "\n",
    "os.makedirs(wavs_dir, exist_ok=True)\n",
    "\n",
    "for tsv_file in ('train.tsv', 'eval.tsv', 'dev.tsv', 'eval0.tsv'):\n",
    "    tsv_path = os.path.join(data_dir, tsv_file)\n",
    "    tsv_url = f'{fae_cv_url}/{tsv_file}'\n",
    "    if not os.path.exists(tsv_path):\n",
    "        print(f'downloading {tsv_url}')\n",
    "        wget.download(tsv_url, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TSV fileas as dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'locale', 'segment', 'isfemale', 'label', 'duration']\n",
    "#\n",
    "trainDF = pd.read_csv(os.path.join(data_dir, 'train.tsv'), sep='\\t', names=colnames)\n",
    "devDF = pd.read_csv(os.path.join(data_dir, 'dev.tsv'), sep='\\t', names=colnames)\n",
    "testDF = pd.read_csv(os.path.join(data_dir, 'eval.tsv'), sep='\\t', names=colnames)\n",
    "evalDF = pd.read_csv(os.path.join(data_dir, 'eval0.tsv'), sep='\\t', names=colnames)\n",
    "\n",
    "Path(wavs_dir).mkdir(parents=True, exist_ok=True)\n",
    "trainDF['wavpath'] = trainDF.path.apply(lambda x : os.path.join(wavs_dir, x.replace('.mp3','.wav')))\n",
    "devDF['wavpath'] = devDF.path.apply(lambda x : os.path.join(wavs_dir, x.replace('.mp3','.wav')))\n",
    "testDF['wavpath'] = testDF.path.apply(lambda x : os.path.join(wavs_dir, x.replace('.mp3','.wav')))\n",
    "evalDF['wavpath'] = evalDF.path.apply(lambda x : os.path.join(wavs_dir, x.replace('.mp3','.wav')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform recordings from MP3 to 16-bit linear-PCM 16kHz wav files.\n",
    "Recordingas are downsampled to 16kHz and gain normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting mp3 --> wav... this may take a couple of minutes...\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "def au_transform(x, tfm=sox.Transformer()):\n",
    "    tfm.set_output_format(rate=16000)\n",
    "    tfm.gain(normalize=True)\n",
    "    in_filepath = os.path.join(clips_dir, x)\n",
    "    out_filepath = os.path.join(wavs_dir, x.replace('.mp3','.wav'))\n",
    "    status = tfm.build(input_filepath=in_filepath, output_filepath=out_filepath)\n",
    "    return status\n",
    "def get_duration(path):\n",
    "    return sox.file_info.duration(path)    \n",
    "#\n",
    "print('Converting mp3 --> wav... this may take a couple of minutes...')\n",
    "MAX_NB_CPU_WORKERS = min(20, int(os.cpu_count() / 4))\n",
    "with Pool(MAX_NB_CPU_WORKERS) as p:\n",
    "    p.map( au_transform,  trainDF.path )\n",
    "    p.map( au_transform,  devDF.path )\n",
    "    p.map( au_transform,  testDF.path )\n",
    "    p.map( au_transform,  evalDF.path )\n",
    "print('...done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the duration of new wav files.... it may take a minute...\n",
      "...done!\n"
     ]
    }
   ],
   "source": [
    "# refresh the duration info after converting to wav as it may have drifted a little.\n",
    "print('Updating the duration of new wav files.... it may take a minute...')\n",
    "trainDF['wav_duration'] = trainDF.wavpath.apply(get_duration)\n",
    "devDF['wav_duration'] = devDF.wavpath.apply(get_duration)\n",
    "testDF['wav_duration'] = testDF.wavpath.apply(get_duration)\n",
    "evalDF['wav_duration'] = evalDF.wavpath.apply(get_duration)\n",
    "print('...done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finetune a Speaker-Recognition pretrained model.\n",
    "\n",
    "#### Repurpose a DNN pretrained for speaker separation to learn an embedding space where accents are separated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create manifest files.\n",
    "\n",
    "Schema:\n",
    "```\n",
    "{\"audio_filepath\": \"\", \"offset\": 0, \"duration\": 0, \"label\": \"\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "train_manifest = os.path.join(data_dir,'manifest-train.json')\n",
    "validation_manifest = os.path.join(data_dir,'manifest-dev.json')\n",
    "test_manifest = os.path.join(data_dir,'manifest-test.json')\n",
    "eval_manifest = os.path.join(data_dir,'manifest-eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_filesL = (train_manifest, validation_manifest, test_manifest, eval_manifest )\n",
    "dfL = (trainDF, devDF, testDF, evalDF)\n",
    "\n",
    "# Rename columns\n",
    "name_mappingD = dict(wavpath='audio_filepath', client_id='speaker', wav_duration='duration')\n",
    "\n",
    "for manifest_filepath,df in zip(manifest_filesL, dfL):\n",
    "    df[['wavpath', 'client_id', 'wav_duration', 'label']].rename(name_mappingD, axis=1).to_json(manifest_filepath, orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Note: All the following steps are just for explanation of each section, but one can use the provided [FAE recognition script](https://github.com/schaltung/NeMo/blob/fae22/examples/speaker_tasks/paralinguistics/speaker_fae_reco.py) to launch training in the command line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The ECAPA model is defined in a config file which declares multiple important sections.\n",
    "\n",
    "They are:\n",
    "\n",
    "1) model: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets, and any other related information\n",
    "\n",
    "2) trainer: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p conf\n",
    "! wget -q -P conf https://raw.githubusercontent.com/schaltung/NeMo/$BRANCH/examples/speaker_tasks/paralinguistics/conf/ecapa_tdnn-finetune-cv.yaml\n",
    "MODEL_CONFIG = 'conf/ecapa_tdnn-finetune-cv.yaml'\n",
    "finetune_config = OmegaConf.load(MODEL_CONFIG)\n",
    "#print(OmegaConf.to_yaml(finetune_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the datasets within the config\n",
    "The config dictionaries called `train_ds`, `validation_ds` and `test_ds` are configurations used to setup the Dataset and DataLoaders of the corresponding config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifest_filepath: ???\n",
      "sample_rate: ${model.sample_rate}\n",
      "labels: null\n",
      "batch_size: 64\n",
      "shuffle: true\n",
      "augmentor:\n",
      "  speed:\n",
      "    prob: 0.5\n",
      "    sr: ${model.sample_rate}\n",
      "    resample_type: kaiser_fast\n",
      "    min_speed_rate: 0.95\n",
      "    max_speed_rate: 1.05\n",
      "\n",
      "manifest_filepath: ???\n",
      "sample_rate: ${model.sample_rate}\n",
      "labels: null\n",
      "batch_size: 128\n",
      "shuffle: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(finetune_config.model.train_ds))\n",
    "print(OmegaConf.to_yaml(finetune_config.model.validation_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often notice that some configs have `???` in place of paths. This is used as a placeholder so that the user can change the value at a later time.\n",
    "\n",
    "Let's add the paths to the manifests to the config above\n",
    "Also, since an4 dataset doesn't have a test set of the same speakers used in training, we will use validation manifest as test manifest for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'manifest_filepath': '???', 'sample_rate': '${model.sample_rate}', 'labels': None, 'batch_size': 128, 'shuffle': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_config.model.train_ds.manifest_filepath = train_manifest\n",
    "finetune_config.model.validation_ds.manifest_filepath = validation_manifest\n",
    "finetune_config.model.pop('test_ds', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Since we are training speaker embedding extractor model for verification we do not add test_ds dataset. To include it add it to config and replace manifest file as \n",
    "`config.model.test_ds.manifest_filepath = test_manifest`\n",
    "\n",
    "Also as we are training on FAE-CV22 dataset, there are 11 accent labels in training, and we need to set this in the decoder config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.label.nunique())\n",
    "finetune_config.model.decoder.num_classes = trainDF.label.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-tune training step using the PyTorch Lightning Trainer\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Finetuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways we can finetune to accent classification. \n",
    "1. Finetuning using a pretrained model published on NGC. \n",
    "2. Finetuning from a PTL checkpoint. \n",
    "\n",
    "We use `ecapa_tdnn-finetune-cv.yaml` config, created to show finetuning on pretrained ECAPA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You may use [finetune-script](https://github.com/schaltung/NeMo/blob/fae/cv22/examples/speaker_tasks/paralinguistics/speaker_fae_finetune.py) to launch training in the command line. Following is just a demonstration of the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one would like to finetune from a PTL checkpoint, `init_from_pretrained_model` in config should be replaced with `init_from_nemo_model` and need to provide the path to checkpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have set up the data and changed the decoder required for finetune, we now just need to create a trainer and start training with a smaller learning rate for fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices:\n",
      "- 1\n",
      "max_epochs: 2\n",
      "max_steps: -1\n",
      "num_nodes: 1\n",
      "accelerator: gpu\n",
      "accumulate_grad_batches: 1\n",
      "deterministic: false\n",
      "enable_checkpointing: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the new trainer object\n",
    "# Let us modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainer_config = finetune_config.trainer\n",
    "trainer_config.accelerator = accelerator\n",
    "\n",
    "trainer_config.pop('strategy',None)\n",
    "\n",
    "# DBG:\n",
    "trainer_config.max_epochs = 2\n",
    "trainer_config.devices = [1]\n",
    "\n",
    "print(OmegaConf.to_yaml(trainer_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "trainer_finetune = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a NeMo Experiment\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:18:28 exp_manager:286] Experiments will be logged at /data1/nemo_experiments/tb/FAE-finetune-CV/2022-09-13_05-18-28\n",
      "[NeMo I 2022-09-13 05:18:28 exp_manager:660] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:18:28 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2274: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\n",
      "      rank_zero_deprecation(\"`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\")\n",
      "    \n",
      "[NeMo W 2022-09-13 05:18:28 exp_manager:900] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/nemo_experiments/tb/FAE-finetune-CV/2022-09-13_05-18-28\n"
     ]
    }
   ],
   "source": [
    "log_dir_finetune = exp_manager(trainer_finetune, finetune_config.get(\"exp_manager\", None))\n",
    "print(log_dir_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:18:49 collections:297] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2022-09-13 05:18:49 collections:301] # 7272 files loaded accounting to # 11 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:18:49 label_models:159] Total number of 11 found in all the manifest files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:18:49 collections:297] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2022-09-13 05:18:49 collections:301] # 7272 files loaded accounting to # 11 labels\n",
      "[NeMo I 2022-09-13 05:18:49 collections:297] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2022-09-13 05:18:49 collections:301] # 916 files loaded accounting to # 11 labels\n",
      "[NeMo I 2022-09-13 05:18:49 features:223] PADDING: 16\n",
      "[NeMo I 2022-09-13 05:18:49 label_models:102] loss is Angular Softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:18:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (TopKClassificationAccuracy). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo W 2022-09-13 05:18:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "speaker_model = EncDecSpeakerLabelModel(cfg=finetune_config.model, \\\n",
    "    trainer=trainer_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:18:54 cloud:56] Found existing object /root/.cache/torch/NeMo/NeMo_1.12.0rc0/ecapa_tdnn/20b7839bda482a0b7d4b3390c337d2bc/ecapa_tdnn.nemo.\n",
      "[NeMo I 2022-09-13 05:18:54 cloud:62] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.12.0rc0/ecapa_tdnn/20b7839bda482a0b7d4b3390c337d2bc/ecapa_tdnn.nemo\n",
      "[NeMo I 2022-09-13 05:18:54 common:910] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:18:55 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/train.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    time_length: 3\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /manifests/noise/rir_noise_manifest.json\n",
      "        prob: 0.5\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 15\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2022-09-13 05:18:55 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/combined_fisher_swbd_voxceleb12_librispeech/dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    time_length: 3\n",
      "    num_workers: 15\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:18:55 features:223] PADDING: 16\n",
      "[NeMo I 2022-09-13 05:18:55 label_models:102] loss is Angular Softmax\n",
      "[NeMo I 2022-09-13 05:18:56 save_restore_connector:243] Model EncDecSpeakerLabelModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.12.0rc0/ecapa_tdnn/20b7839bda482a0b7d4b3390c337d2bc/ecapa_tdnn.nemo.\n",
      "[NeMo I 2022-09-13 05:18:56 modelPT:974] Model checkpoint partially restored from pretrained checkpoint with name `ecapa_tdnn`\n",
      "[NeMo I 2022-09-13 05:18:56 modelPT:976] The following parameters were excluded from loading from pretrained checkpoint with name `ecapa_tdnn` : ['decoder.final.weight']\n",
      "[NeMo I 2022-09-13 05:18:56 modelPT:979] Make sure that this is what you wanted!\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained model from NGC.\n",
    "speaker_model.maybe_init_from_pretrained_checkpoint(finetune_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the config, we keep weights of preprocessor and encoder, and attach a new decoder as mentioned in above section to match num of classes of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:19:21 modelPT:597] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        capturable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 1e-05\n",
      "        maximize: False\n",
      "        weight_decay: 0.0002\n",
      "    )\n",
      "[NeMo I 2022-09-13 05:19:21 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f0561735a90>\" \n",
      "    will be used during training (effective maximum steps = 228) - \n",
      "    Parameters : \n",
      "    (warmup_ratio: 0.5\n",
      "    min_lr: 0\n",
      "    max_steps: 228\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type                              | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | preprocessor    | AudioToMelSpectrogramPreprocessor | 0     \n",
      "1 | encoder         | ECAPAEncoder                      | 18.1 M\n",
      "2 | decoder         | SpeakerDecoder                    | 2.8 M \n",
      "3 | loss            | AngularSoftmaxLoss                | 0     \n",
      "4 | _accuracy       | TopKClassificationAccuracy        | 0     \n",
      "5 | _macro_accuracy | Accuracy                          | 0     \n",
      "6 | _auroc          | AUROC                             | 0     \n",
      "----------------------------------------------------------------------\n",
      "20.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.9 M    Total params\n",
      "83.676    Total estimated model params size (MB)\n",
      "2022-09-13 05:19:21.053699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-13 05:19:21.328123: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-13 05:19:22.001537: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-09-13 05:19:22.001681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.8/site-packages/torch/lib:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-09-13 05:19:22.001693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015350580215454102,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537c702beed343c2831cbdce6bc11cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:19:23 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2022-09-13 05:19:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:19:30 label_models:353] val_loss: 12.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:19:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/module.py:555: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "      value = torch.tensor(value, device=self.device)\n",
      "    \n",
      "[NeMo W 2022-09-13 05:19:30 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:225: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01520085334777832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8978044cdd413386f98ff669b71bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "[NeMo W 2022-09-13 05:19:32 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/core/module.py:555: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "      value = torch.tensor(value, device=self.device)\n",
      "    \n",
      "[NeMo W 2022-09-13 05:19:32 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018373489379882812,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effc47198d864d3886fd810429ae0c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:21:06 label_models:353] val_loss: 8.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 114: 'val_loss' reached 8.51920 (best 8.51920), saving model to '/data1/nemo_experiments/tb/FAE-finetune-CV/2022-09-13_05-18-28/checkpoints/FAE-finetune-CV--val_loss=8.5192-epoch=0.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0166018009185791,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e16194d2554426aa56765267c5b1db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:23:07 label_models:353] val_loss: 7.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 228: 'val_loss' reached 7.74707 (best 7.74707), saving model to '/data1/nemo_experiments/tb/FAE-finetune-CV/2022-09-13_05-18-28/checkpoints/FAE-finetune-CV--val_loss=7.7471-epoch=1.ckpt' as top 3\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer_finetune.fit(speaker_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving .nemo file\n",
    "Now we can save the whole config and model parameters in a single .nemo and we can anytime restore from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nemo_path = os.path.join(log_dir_finetune, \"ecapa_fae-cv22-finetune.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model.save_to(model_nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualize Embeddings with TensorBoard Projector\n",
    "\n",
    "#### Task: Foreign Accent English classification\n",
    "\n",
    "#### Corpus: FAE-CV22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.asr.models import EncDecSpeakerLabelModel\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Speaker-Accent Model\n",
    "Load NeMo file and manifest files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_nemo_path = model_nemo_path\n",
    "#\n",
    "# Data manifests\n",
    "test_manifest_filepath = test_manifest\n",
    "eval_manifest_filepath = eval_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define In/Out filepaths\n",
    "# Select below as needed.\n",
    "#\n",
    "conf = OmegaConf.create()\n",
    "conf[\"data\"] = {}\n",
    "# IN:\n",
    "test_title = f'{model_nemo_path.split(\"/\")[-1]}\\n{os.path.basename(test_manifest_filepath)}'\n",
    "conf.data[\"test\"] = dict(\\\n",
    "    manifest_path=test_manifest_filepath, \\\n",
    "    title=test_title\n",
    "    )\n",
    "#\n",
    "eval_title = f'{model_nemo_path.split(\"/\")[-1]}\\n{os.path.basename(eval_manifest_filepath)}'\n",
    "conf.data[\"eval\"] = dict(\\\n",
    "    manifest_path=eval_manifest_filepath,\n",
    "    title=eval_title\n",
    "    )\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  test:\n",
      "    manifest_path: data/manifest-test.json\n",
      "    title: 'ecapa_fae-cv22-finetune.nemo\n",
      "\n",
      "      manifest-test.json'\n",
      "  eval:\n",
      "    manifest_path: data/manifest-eval.json\n",
      "    title: 'ecapa_fae-cv22-finetune.nemo\n",
      "\n",
      "      manifest-eval.json'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1: Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:24:55 modelPT:142] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: data/manifest-train.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - australia\n",
      "    - canada\n",
      "    - england\n",
      "    - hongkong\n",
      "    - india\n",
      "    - ireland\n",
      "    - malaysia\n",
      "    - newzealand\n",
      "    - philippines\n",
      "    - scotland\n",
      "    - us\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    augmentor:\n",
      "      speed:\n",
      "        prob: 0.5\n",
      "        sr: 16000\n",
      "        resample_type: kaiser_fast\n",
      "        min_speed_rate: 0.95\n",
      "        max_speed_rate: 1.05\n",
      "    \n",
      "[NeMo W 2022-09-13 05:24:55 modelPT:149] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: data/manifest-dev.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:24:55 features:223] PADDING: 16\n",
      "[NeMo I 2022-09-13 05:24:56 label_models:102] loss is Angular Softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-09-13 05:24:56 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (TopKClassificationAccuracy). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo W 2022-09-13 05:24:56 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:24:56 save_restore_connector:243] Model EncDecSpeakerLabelModel was successfully restored from /data1/nemo_experiments/tb/FAE-finetune-CV/2022-09-13_05-18-28/ecapa_fae-cv22-finetune.nemo.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load DNN.\n",
    "speaker_model_ = EncDecSpeakerLabelModel.restore_from(model_nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Extract Embeddings\n",
    "\n",
    "#### Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-09-13 05:26:14 collections:297] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2022-09-13 05:26:14 collections:301] # 916 files loaded accounting to # 11 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stft input and window must be on the same device but got self on cpu and window on cuda:1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m MANIFEST_FILEPATH \u001b[39m=\u001b[39m conf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtest\u001b[39m.\u001b[39mmanifest_path\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m (\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     test_embs,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     test_logits,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     test_ref_labels,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     test_idx2labD,\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m ) \u001b[39m=\u001b[39m EncDecSpeakerLabelModel\u001b[39m.\u001b[39;49mget_batch_embeddings(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     speaker_model\u001b[39m=\u001b[39;49mspeaker_model_,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     manifest_filepath\u001b[39m=\u001b[39;49mMANIFEST_FILEPATH,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     sample_rate\u001b[39m=\u001b[39;49m\u001b[39m16000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     device\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e746f6e696f6661652d6661654e654d6f2d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6835227d7d/workspace/nemo/tutorials/speaker_tasks/FAE_Speaker_Accent_Classification.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m test_embs \u001b[39m=\u001b[39m test_embs \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(test_embs, \u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/asr/models/label_models.py:499\u001b[0m, in \u001b[0;36mEncDecSpeakerLabelModel.get_batch_embeddings\u001b[0;34m(speaker_model, manifest_filepath, batch_size, sample_rate, device)\u001b[0m\n\u001b[1;32m    497\u001b[0m     test_batch \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m test_batch]\n\u001b[1;32m    498\u001b[0m audio_signal, audio_signal_len, labels, _ \u001b[39m=\u001b[39m test_batch\n\u001b[0;32m--> 499\u001b[0m logits, embs \u001b[39m=\u001b[39m speaker_model\u001b[39m.\u001b[39;49mforward(input_signal\u001b[39m=\u001b[39;49maudio_signal, input_signal_length\u001b[39m=\u001b[39;49maudio_signal_len)\n\u001b[1;32m    501\u001b[0m all_logits\u001b[39m.\u001b[39mextend(logits\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    502\u001b[0m all_labels\u001b[39m.\u001b[39mextend(labels\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py:1084\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m instance\u001b[39m.\u001b[39m_validate_input_types(input_types\u001b[39m=\u001b[39minput_types, ignore_collections\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_collections, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m outputs \u001b[39m=\u001b[39m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1086\u001b[0m instance\u001b[39m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1087\u001b[0m     output_types\u001b[39m=\u001b[39moutput_types, ignore_collections\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_collections, out_objects\u001b[39m=\u001b[39moutputs\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/asr/models/label_models.py:293\u001b[0m, in \u001b[0;36mEncDecSpeakerLabelModel.forward\u001b[0;34m(self, input_signal, input_signal_length)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m@typecheck\u001b[39m()\n\u001b[1;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_signal, input_signal_length):\n\u001b[0;32m--> 293\u001b[0m     processed_signal, processed_signal_len \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocessor(\n\u001b[1;32m    294\u001b[0m         input_signal\u001b[39m=\u001b[39;49minput_signal, length\u001b[39m=\u001b[39;49minput_signal_length,\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec_augmentation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    298\u001b[0m         processed_signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec_augmentation(input_spec\u001b[39m=\u001b[39mprocessed_signal, length\u001b[39m=\u001b[39mprocessed_signal_len)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/core/classes/common.py:1084\u001b[0m, in \u001b[0;36mtypecheck.__call__\u001b[0;34m(self, wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m instance\u001b[39m.\u001b[39m_validate_input_types(input_types\u001b[39m=\u001b[39minput_types, ignore_collections\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_collections, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m outputs \u001b[39m=\u001b[39m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1086\u001b[0m instance\u001b[39m.\u001b[39m_attach_and_validate_output_types(\n\u001b[1;32m   1087\u001b[0m     output_types\u001b[39m=\u001b[39moutput_types, ignore_collections\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_collections, out_objects\u001b[39m=\u001b[39moutputs\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/asr/modules/audio_preprocessing.py:85\u001b[0m, in \u001b[0;36mAudioPreprocessor.forward\u001b[0;34m(self, input_signal, length)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m@typecheck\u001b[39m()\n\u001b[1;32m     83\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_signal, length):\n\u001b[0;32m---> 85\u001b[0m     processed_signal, processed_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_features(input_signal, length)\n\u001b[1;32m     87\u001b[0m     \u001b[39mreturn\u001b[39;00m processed_signal, processed_length\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/asr/modules/audio_preprocessing.py:268\u001b[0m, in \u001b[0;36mAudioToMelSpectrogramPreprocessor.get_features\u001b[0;34m(self, input_signal, length)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_features\u001b[39m(\u001b[39mself\u001b[39m, input_signal, length):\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeaturizer(input_signal, length)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/asr/parts/preprocessing/features.py:350\u001b[0m, in \u001b[0;36mFilterbankFeatures.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39m# disable autocast to get full range of stft values\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 350\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstft(x)\n\u001b[1;32m    352\u001b[0m \u001b[39m# torch returns real, imag; so convert to magnitude\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m# guard is needed for sqrt if grads are passed through\u001b[39;00m\n\u001b[1;32m    354\u001b[0m guard \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_grads \u001b[39melse\u001b[39;00m CONSTANT\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nemo/collections/asr/parts/preprocessing/features.py:242\u001b[0m, in \u001b[0;36mFilterbankFeatures.__init__.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    240\u001b[0m window_tensor \u001b[39m=\u001b[39m window_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwin_length, periodic\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m window_fn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mwindow\u001b[39m\u001b[39m\"\u001b[39m, window_tensor)\n\u001b[0;32m--> 242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstft \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: torch\u001b[39m.\u001b[39;49mstft(\n\u001b[1;32m    243\u001b[0m     x,\n\u001b[1;32m    244\u001b[0m     n_fft\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_fft,\n\u001b[1;32m    245\u001b[0m     hop_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhop_length,\n\u001b[1;32m    246\u001b[0m     win_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwin_length,\n\u001b[1;32m    247\u001b[0m     center\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m exact_pad \u001b[39melse\u001b[39;49;00m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    248\u001b[0m     window\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat),\n\u001b[1;32m    249\u001b[0m     return_complex\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39m=\u001b[39m normalize\n\u001b[1;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog \u001b[39m=\u001b[39m log\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/functional.py:606\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    605\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39msignal_dim:])\n\u001b[0;32m--> 606\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mstft(\u001b[39minput\u001b[39;49m, n_fft, hop_length, win_length, window,  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m    607\u001b[0m                 normalized, onesided, return_complex)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stft input and window must be on the same device but got self on cpu and window on cuda:1"
     ]
    }
   ],
   "source": [
    "# STEP 2.1: Run Test.\n",
    "#\n",
    "MANIFEST_FILEPATH = conf.data.test.manifest_path\n",
    "#\n",
    "(\n",
    "    test_embs,\n",
    "    test_logits,\n",
    "    test_ref_labels,\n",
    "    test_idx2labD,\n",
    ") = EncDecSpeakerLabelModel.get_batch_embeddings(\n",
    "    speaker_model=speaker_model_,\n",
    "    manifest_filepath=MANIFEST_FILEPATH,\n",
    "    batch_size=32,\n",
    "    sample_rate=16000,\n",
    "    device=accelerator,\n",
    ")\n",
    "test_embs = test_embs / (np.linalg.norm(test_embs, ord=2, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2.1: Run Evaluation.\n",
    "#\n",
    "MANIFEST_FILEPATH = conf.data.eval.manifest_path\n",
    "#\n",
    "(\n",
    "    eval_embs,\n",
    "    eval_logits,\n",
    "    eval_ref_labels,\n",
    "    eval_idx2labD,\n",
    ") = EncDecSpeakerLabelModel.get_batch_embeddings(\n",
    "    speaker_model=speaker_model,\n",
    "    manifest_filepath=MANIFEST_FILEPATH,\n",
    "    batch_size=32,\n",
    "    sample_rate=16000,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "eval_embs = eval_embs / (np.linalg.norm(eval_embs, ord=2, axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Visualization of the Embedding Space\n",
    "\n",
    "#### TensorBoard Projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch this to True if you really want to create a new entry in TB.\n",
    "WRITE2TB = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Eval Data Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather vector data\n",
    "vectors = eval_embs\n",
    "metadata = [eval_idx2labD[x] for x in eval_ref_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SummaryWritter\n",
    "logdir = f\"{LOG_DIR}/SelfEmbsEvalCV_{datetime.datetime.now().isoformat()}\".replace(\n",
    "    \":\", \"-\"\n",
    ")\n",
    "if WRITE2TB:\n",
    "    writer = SummaryWriter(logdir)\n",
    "    writer = SummaryWriter(logdir)\n",
    "    writer.add_embedding(vectors, metadata)\n",
    "    writer.close()\n",
    "print(f\"Wrote {vectors.shape[0]} embeddings in a {vectors.shape[1]}-dim space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "origMetaDF = pd.read_csv(\n",
    "    f\"{BASE_DIR}/220831-Finetune+Eval-CV-validated/CV-self-recordings/data.tsv\", sep=\",\"\n",
    ")\n",
    "# origMetaDF\n",
    "# origMetaDF.columns=['client_id', 'path', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accents', 'locale', 'segment', 'label']\n",
    "#\n",
    "_ = pd.merge(left=manifestDF, right=origMetaDF, how=\"left\", on=[\"path\", \"label\"])\n",
    "_[\"speaker\"] = _.client_id\n",
    "_[\"collection\"] = \"iact\"\n",
    "metadataLOL = _[\n",
    "    [\"label\", \"gender\", \"age\", \"speaker\", \"sentence\", \"collection\"]\n",
    "].values.tolist()\n",
    "# metadataLOL = manifestDF[['label', 'path']].values.tolist()\n",
    "# metadataLOL\n",
    "MANIFEST_FILEPATH = conf.data.eval0.manifest_path\n",
    "manifest0DF = pd.read_json(MANIFEST_FILEPATH, lines=True)\n",
    "manifest0DF[\"path\"] = manifest0DF.audio_filepath.apply(\n",
    "    lambda x: x.split(\"/\")[-1]\n",
    ").apply(lambda x: x.replace(\".wav\", \".mp3\"))\n",
    "\n",
    "origMeta0DF = pd.read_csv(\n",
    "    f\"{BASE_DIR}/220823-Finetune-CV-validated/validated-label-dur.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    ")\n",
    "origMeta0DF.columns = [\n",
    "    \"client_id\",\n",
    "    \"path\",\n",
    "    \"sentence\",\n",
    "    \"up_votes\",\n",
    "    \"down_votes\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"accents\",\n",
    "    \"locale\",\n",
    "    \"segment\",\n",
    "    \"label\",\n",
    "    \"duration\",\n",
    "]\n",
    "#\n",
    "_ = pd.merge(left=manifest0DF, right=origMeta0DF, how=\"left\", on=[\"path\", \"label\"])\n",
    "_[\"speaker\"] = _.client_id.apply(lambda x: x[-4:])\n",
    "_[\"collection\"] = \"mozilla\"\n",
    "metadata0LOL = _[\n",
    "    [\"label\", \"gender\", \"age\", \"speaker\", \"sentence\", \"collection\"]\n",
    "].values.tolist()\n",
    "\n",
    "metadataLOL += metadata0LOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.concatenate((eval_embs, eval0_embs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather vector data\n",
    "vectors = embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SummaryWritter\n",
    "logdir = f\"{LOG_DIR}/SelfEmbsEvalCV_{datetime.datetime.now().isoformat()}\".replace(\n",
    "    \":\", \"-\"\n",
    ")\n",
    "WRITE2TB = True\n",
    "if WRITE2TB:\n",
    "    writer = SummaryWriter(logdir)\n",
    "    writer.add_embedding(\n",
    "        vectors,\n",
    "        metadataLOL,\n",
    "        metadata_header=[\"label\", \"gender\", \"age\", \"speaker\", \"sentence\", \"collection\"],\n",
    "        tag=\"SelfEmbsEvalCV\",\n",
    "    )\n",
    "    writer.close()\n",
    "print(\n",
    "    f\"{logdir}\\nWrote {vectors.shape[0]} embeddings in a {vectors.shape[1]}-dim space.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
